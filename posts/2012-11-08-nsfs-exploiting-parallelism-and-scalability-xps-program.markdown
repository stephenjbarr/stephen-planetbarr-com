---
layout: post
title: "NSF's 'Exploiting Parallelism and Scalability' (XPS) Program"
description: "NSF's 'Exploiting Parallelism and Scalability' (XPS) Program"
date: 2012-11-08 16:40
comments: true
tags: computer-science, scheduling
---


<img src="https://s3.amazonaws.com/stevejb_blog_content/Titan_2012-P03156_3000-640x391.jpg">
*The above picture is of Titan, the world's new fastest supercomputer. It is located at the [Oak Ridge National Laboratory](http://www.ornl.gov/). Photo source: [Ars Technica](http://arstechnica.com/information-technology/2012/10/doe-flips-switch-on-titan-worlds-newest-fastest-supercomputer/).*


The NSF has announced a program entitled 'Exploiting Parallelism and Scalability' (XPS). The purpose of this program is to bring about the necessary innovations in programming languages, scheduling algorithms, compilers, and all other levels of the hardware and software stack involved in parallel computing. Having been a user of parallel computing since my early teens, when I worked on my first [Beowulf cluster](http://en.wikipedia.org/wiki/Beowulf_cluster) (definitely an outdated term) when I was a teenager. Having used MPI, Hadoop, and even my own [SQS](http://aws.amazon.com/sqs/)-based scheduling system, there is certainly a ton of room for improvement in both the development and the execution of the parallel programs.

From the software development side, the development environment needs to be completely built around parallel execution in a heterogeneous environment. Debugging parallel software is quite difficult. On the scheduling side, I can see a much more tight integration of economics with development time. The development environment should be quoting you prices for CPU time. The job scheduler should take into account the price of electricity when scheduling jobs. The scheduler should be using machine learning to understand the health of the compute nodes, understanding the dynamics in the network bandwidth, and perhaps even sending parts of the jobs to other clusters according to some bidding system. There are many pieces of the puzzle that exist, but there needs to be much more innovation. Exascale computing is a big part of the future, and we as scientists need to make it happen.

## Why this is important ##
Exascale computing is what is going to enable this world to transition into the much smarter, much more interconnected world of tomorrow. In that world, machine learning algorithms are helping each of us optimize our lives. The vast amounts of data that we will be bombarded with will be intelligently filtered allowing humans to get to the heart of any problem. Working with high performance computers and artificial intelligences will be integral to solving the complex problems of the future. In fact, our very thinking may be augmented directly by supercomputers.

## Links ##

+ Blog post on the [CCC blog](http://www.cccblog.org/2012/10/23/nsf-announces-exploiting-parallelism-and-scalability-xps-program/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+cccblog%2FwDnv+%28CCC+Blog%29). 
+ [NSF Program Solicitation](http://www.nsf.gov/pubs/2013/nsf13507/nsf13507.htm?WT.mc_id=USNSF_25&WT.mc_ev=click)
+ Very relevant talk in informs from [Alan Gara](http://meetings2.informs.org/phoenix2012/plenaries.html#gara) (I need to post my notes from this talk)
+ [Slides](http://www.physik.uni-regensburg.de/forschung/wettig/workshops/APQ_April2010/talks/20100414%20lQCD%20RegensburgSteinmacher-Burowv07.pdf) on the hardware side of the path to exascale
+ [DOE flips switch on Titan, world's newest fastest supercomputer](http://arstechnica.com/information-technology/2012/10/doe-flips-switch-on-titan-worlds-newest-fastest-supercomputer/)


